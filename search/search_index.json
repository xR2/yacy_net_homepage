{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"body { background-image: url(\"img/YaCy_Network_Dark.png\"); background-repeat:no-repeat; background-size: 100%; } Decentralized Web Search Web Search Engine Software YaCy is free software for your own search engine. Join a community of search engines or make your own search portal! There are these three use cases you can choose from: P2P Mode Web Search by the people, for the people: decentralized, all users are equal, no central, no search request storage, shared index. Your Search Portal Your YaCy installation is independent from other peers. Define your own web index and starting your own web crawl. Intranet Search Create a search portal for your intranet or web pages or your (shared) file system. function setSpace() { w = window.innerWidth || document.documentElement.clientWidth || doc.getElementsByTagName('body')[0].clientWidth; margin = w * 0.4 - 360; if (margin < -40) { document.body.style.backgroundImage = 'none'; document.body.style.backgroundImage = 'url(\"img/YaCy_Network_Dark_Small.png\")'; margin = -40; } jumbotron = document.getElementsByClassName('jumbotron')[0]; jumbotron.style = 'background: none; margin-bottom: ' + margin + 'px;' c1 = document.getElementById(\"mode-portal\").scrollHeight; if (document.readyState === 'complete') { document.getElementById(\"mode-p2p\").style.height = c1 + \"px\"; document.getElementById(\"mode-intranet\").style.height = c1 + \"px\"; } } setSpace(); window.onresize = setSpace; Decentralization Imagine if, rather than relying on the proprietary software of a large professional search engine operator, your search engine was run by many private computers which aren't under the control of any one company or individual. Well, that's what YaCy does! Here is a live image of the YaCy network: Installation is easy! The installation takes only three minutes. Just download the release, decompress the package and run the start script. Next Steps: Download and Install YaCy Watch screenshots and tutorial movies Try out the YaCy Demo Peer","title":"Home"},{"location":"#web-search-engine-software","text":"YaCy is free software for your own search engine. Join a community of search engines or make your own search portal! There are these three use cases you can choose from:","title":"Web Search Engine Software"},{"location":"#decentralization","text":"Imagine if, rather than relying on the proprietary software of a large professional search engine operator, your search engine was run by many private computers which aren't under the control of any one company or individual. Well, that's what YaCy does! Here is a live image of the YaCy network:","title":"Decentralization"},{"location":"#installation-is-easy","text":"The installation takes only three minutes. Just download the release, decompress the package and run the start script. Next Steps: Download and Install YaCy Watch screenshots and tutorial movies Try out the YaCy Demo Peer","title":"Installation is easy!"},{"location":"community/","text":"","title":"Community"},{"location":"demonstration_tutorial_screenshot/","text":"YaCy Demonstration We have screenshots, tutorial movies, and a live demonstration. If you want to jump to the section where we show where we took the beautiful teaser image from, jump to the peer-to-peer network activity dashboard ! Improve this doc Screenshots .. the best way to show what you get .. Search Results This is the search result page using peer-to-peer data: Search result pages can be configured: We provide a wide range of options to integrate a search page into an existing environment: There are many pre-defined themes and an easy way to configure your own colour schema: Search result ranking can be configured: Crawling Before search resuls can be provided, a web crawl must be done to harvest the required document corpus. There is a easy-to-use simple crawl start service which takes almost only the starting URL as input: A running crawl can be monitored in many ways, here is the crawler status dashboard: For more complex harvesting tasks, here is the Expert Crawl Start service: Data Analysis A running YaCy instance has many management tasks runnning and it creates a lot of data. To visualize that data about itself and the data it harvested from the internet, there are a lot of dashboards and monitoring features. The status page which informs about the current activities of your YaCy instance: Crawled web pages can be reconstructed and browsed with the host browser where you see the internal link structure and all kind of details of the parsing results: While the host browser reveals the structure of documents within all of the single domains, the web strucure service shows how domains are linked to each other: Configuration YaCy can be configured in every detail using the web interface. You can use a configuration file and also edit details inside of that file, but a web front-end is much more convenient. Here you see the use case configuration. This is actually the first service which YaCy shows to you because the first thing you have to do is to choose which use case you are prefering: Peer-to-Peer operation, your own Search Portal, or Intranet Indexing: The main purpose of YaCy is to aquire a lot of data from the web and make them searchable. Consequently, this uses a lot of disk space. We make the resources which YaCy takes configurable with the following service. YaCy loves to use more RAM if available, give it more RAM and it performes better! Network Activity YaCy makes a lot of network connections and it is transparent about doing that. In case you are running YaCy in peer-to-peer operation, you can watch the network graphic here: The peer-to-peer network can be investigated in detail in a table: Every connection that YaCy does, peer-to-peer or crawling, is monitored as well: Tutorial Movies We have a youtube channel! Yes, thats a google service - therefore all videos embedded here are mouseover-activated, that means as long as you do not touch them, there is no referrer appearing at a G server. These videos are short and most have nice music from Shiryu Music in the background. Shiryu is just great and inspiring! Subscribe to the YaCy Tutorial Youtube channel embed01 = \"<iframe width='720' height='405' src='//www.youtube.com/embed/CFwebavBU0s?list=UUvy0FJxqOAlSZ2VXskej79Q' frameborder='0' allowfullscreen></iframe>\" embed02 = \"<iframe width='720' height='405' src='//www.youtube.com/embed/XDoVNzOMoIo?list=UUvy0FJxqOAlSZ2VXskej79Q' frameborder='0' allowfullscreen></iframe>\" embed03 = \"<iframe width='720' height='405' src='//www.youtube.com/embed/iqJuf_EA1UE?list=UUvy0FJxqOAlSZ2VXskej79Q' frameborder='0' allowfullscreen></iframe>\" embed04 = \"<iframe width='720' height='405' src='//www.youtube.com/embed/t5y5MmnmraA?list=UUvy0FJxqOAlSZ2VXskej79Q' frameborder='0' allowfullscreen></iframe>\" embed05 = \"<iframe width='720' height='405' src='//www.youtube.com/embed/UUAylWYqzac?list=UUvy0FJxqOAlSZ2VXskej79Q' frameborder='0' allowfullscreen></iframe>\" embed06 = \"<iframe width='720' height='405' src='//www.youtube.com/embed/hGwjllUdjU0?list=UUvy0FJxqOAlSZ2VXskej79Q' frameborder='0' allowfullscreen></iframe>\" embed07 = \"<iframe width='720' height='405' src='//www.youtube.com/embed/o3-v2oaXSa0?list=UUvy0FJxqOAlSZ2VXskej79Q' frameborder='0' allowfullscreen></iframe>\" Demo: YaCy Installation in Windows Please install Java 8 first, the automatic Java installation within YaCy does not work any more. Demo: YaCy Installation on a Mac Demo: YaCy Installation in Ubuntu and any other Linux Systems Instead of Java-7 now please install Java-8 Demo: Basic Configuration of YaCy and Release Updates Demo: Start a Web Crawl with YaCy Demo: Secret Agents with YaCy RSS Feed Reader Demo: Location Search with YaCy and Opensteetmap and Geonames Live Demonstration Well, YaCy is all about running your own search engine, not using a central one. Therefore we highly recommend you download and try out your own YaCy instance instead of trying this demonstration. Therefore, please consider this demonstration not as production-grade search portal, just as help to find out what YaCy is and how it looks. We provide here a link to the administration page of the test instance where you can also find your way to the search page. Administration functions are disabled but you are able to browse through the services without a password: YaCy Demo Peer Click here to operate a YaCy Demo Peer","title":"Demo"},{"location":"demonstration_tutorial_screenshot/#yacy-demonstration","text":"We have screenshots, tutorial movies, and a live demonstration. If you want to jump to the section where we show where we took the beautiful teaser image from, jump to the peer-to-peer network activity dashboard ! Improve this doc","title":"YaCy Demonstration"},{"location":"demonstration_tutorial_screenshot/#screenshots","text":".. the best way to show what you get ..","title":"Screenshots"},{"location":"demonstration_tutorial_screenshot/#search-results","text":"This is the search result page using peer-to-peer data: Search result pages can be configured: We provide a wide range of options to integrate a search page into an existing environment: There are many pre-defined themes and an easy way to configure your own colour schema: Search result ranking can be configured:","title":"Search Results"},{"location":"demonstration_tutorial_screenshot/#crawling","text":"Before search resuls can be provided, a web crawl must be done to harvest the required document corpus. There is a easy-to-use simple crawl start service which takes almost only the starting URL as input: A running crawl can be monitored in many ways, here is the crawler status dashboard: For more complex harvesting tasks, here is the Expert Crawl Start service:","title":"Crawling"},{"location":"demonstration_tutorial_screenshot/#data-analysis","text":"A running YaCy instance has many management tasks runnning and it creates a lot of data. To visualize that data about itself and the data it harvested from the internet, there are a lot of dashboards and monitoring features. The status page which informs about the current activities of your YaCy instance: Crawled web pages can be reconstructed and browsed with the host browser where you see the internal link structure and all kind of details of the parsing results: While the host browser reveals the structure of documents within all of the single domains, the web strucure service shows how domains are linked to each other:","title":"Data Analysis"},{"location":"demonstration_tutorial_screenshot/#configuration","text":"YaCy can be configured in every detail using the web interface. You can use a configuration file and also edit details inside of that file, but a web front-end is much more convenient. Here you see the use case configuration. This is actually the first service which YaCy shows to you because the first thing you have to do is to choose which use case you are prefering: Peer-to-Peer operation, your own Search Portal, or Intranet Indexing: The main purpose of YaCy is to aquire a lot of data from the web and make them searchable. Consequently, this uses a lot of disk space. We make the resources which YaCy takes configurable with the following service. YaCy loves to use more RAM if available, give it more RAM and it performes better!","title":"Configuration"},{"location":"demonstration_tutorial_screenshot/#network-activity","text":"YaCy makes a lot of network connections and it is transparent about doing that. In case you are running YaCy in peer-to-peer operation, you can watch the network graphic here: The peer-to-peer network can be investigated in detail in a table: Every connection that YaCy does, peer-to-peer or crawling, is monitored as well:","title":"Network Activity"},{"location":"demonstration_tutorial_screenshot/#tutorial-movies","text":"We have a youtube channel! Yes, thats a google service - therefore all videos embedded here are mouseover-activated, that means as long as you do not touch them, there is no referrer appearing at a G server. These videos are short and most have nice music from Shiryu Music in the background. Shiryu is just great and inspiring! Subscribe to the YaCy Tutorial Youtube channel embed01 = \"<iframe width='720' height='405' src='//www.youtube.com/embed/CFwebavBU0s?list=UUvy0FJxqOAlSZ2VXskej79Q' frameborder='0' allowfullscreen></iframe>\" embed02 = \"<iframe width='720' height='405' src='//www.youtube.com/embed/XDoVNzOMoIo?list=UUvy0FJxqOAlSZ2VXskej79Q' frameborder='0' allowfullscreen></iframe>\" embed03 = \"<iframe width='720' height='405' src='//www.youtube.com/embed/iqJuf_EA1UE?list=UUvy0FJxqOAlSZ2VXskej79Q' frameborder='0' allowfullscreen></iframe>\" embed04 = \"<iframe width='720' height='405' src='//www.youtube.com/embed/t5y5MmnmraA?list=UUvy0FJxqOAlSZ2VXskej79Q' frameborder='0' allowfullscreen></iframe>\" embed05 = \"<iframe width='720' height='405' src='//www.youtube.com/embed/UUAylWYqzac?list=UUvy0FJxqOAlSZ2VXskej79Q' frameborder='0' allowfullscreen></iframe>\" embed06 = \"<iframe width='720' height='405' src='//www.youtube.com/embed/hGwjllUdjU0?list=UUvy0FJxqOAlSZ2VXskej79Q' frameborder='0' allowfullscreen></iframe>\" embed07 = \"<iframe width='720' height='405' src='//www.youtube.com/embed/o3-v2oaXSa0?list=UUvy0FJxqOAlSZ2VXskej79Q' frameborder='0' allowfullscreen></iframe>\"","title":"Tutorial Movies"},{"location":"demonstration_tutorial_screenshot/#demo-yacy-installation-in-windows","text":"Please install Java 8 first, the automatic Java installation within YaCy does not work any more.","title":"Demo: YaCy Installation in Windows"},{"location":"demonstration_tutorial_screenshot/#demo-yacy-installation-on-a-mac","text":"","title":"Demo: YaCy Installation on a Mac"},{"location":"demonstration_tutorial_screenshot/#demo-yacy-installation-in-ubuntu-and-any-other-linux-systems","text":"Instead of Java-7 now please install Java-8","title":"Demo: YaCy Installation in Ubuntu and any other Linux Systems"},{"location":"demonstration_tutorial_screenshot/#demo-basic-configuration-of-yacy-and-release-updates","text":"","title":"Demo: Basic Configuration of YaCy and Release Updates"},{"location":"demonstration_tutorial_screenshot/#demo-start-a-web-crawl-with-yacy","text":"","title":"Demo: Start a Web Crawl with YaCy"},{"location":"demonstration_tutorial_screenshot/#demo-secret-agents-with-yacy-rss-feed-reader","text":"","title":"Demo: Secret Agents with YaCy RSS Feed Reader"},{"location":"demonstration_tutorial_screenshot/#demo-location-search-with-yacy-and-opensteetmap-and-geonames","text":"","title":"Demo: Location Search with YaCy and Opensteetmap and Geonames"},{"location":"demonstration_tutorial_screenshot/#live-demonstration","text":"Well, YaCy is all about running your own search engine, not using a central one. Therefore we highly recommend you download and try out your own YaCy instance instead of trying this demonstration. Therefore, please consider this demonstration not as production-grade search portal, just as help to find out what YaCy is and how it looks. We provide here a link to the administration page of the test instance where you can also find your way to the search page. Administration functions are disabled but you are able to browse through the services without a password:","title":"Live Demonstration"},{"location":"demonstration_tutorial_screenshot/#yacy-demo-peer","text":"Click here to operate a YaCy Demo Peer","title":"YaCy Demo Peer"},{"location":"download_installation/","text":"Download and Installation of YaCy YaCy is available as packages for Windows, Macintosh and Debian. You can also install YaCy on any other operation system using a tarball. YaCy needs Java 8, OpenJDK 8 is recommended. Improve this doc Please Support YaCy YaCy is libre software - licensed GPL-2+. Downloads are provided free, with the option to donate: Private User Please donate a one-time sum 5 \u20ac 25 \u20ac 50 \u20ac Commercial Users ..Universities and generous private users: please subscribe to GitHub Sponsors or You may also download and try out YaCy first and come back later to donate. Please help! Installation Installation is very easy on any system. If it looks complex here it's just because we give directions in great detail. YaCy for Windows Install Java 8 from https://adoptopenjdk.net/ . Download YaCy for Windows from https://download.yacy.net/yacy_v1.922_20191014_9966.exe Install YaCy by double-clicking the downloaded installer. When the message \"Windows protected your PC (in Windows 10) appears, klick \"More info\", then \"Run anyway\". When asked \"Do you want to allow this app from an unknown publisher to make changes to your device\", klick \"yes\". Within the YaCy Installer, klick \"Next\", \"I Agree\", \"Next\", \"Install\", \"Finish\". To run YaCy, double-click the YaCy app icon (which may be on your desktop). YaCy is then running on port 8090 on your machine. Open http://localhost:8090 in your web-browser. YaCy for Linux and other OS You must install Java 8 on your computer if you don't have it already, and start the YaCy start shell script. YaCy can run under a normal user account, no root rights are needed if Java 8 is already installed. Install JDK 8 (Oracle or preferably OpenJDK). You get Java for Linux from https://adoptopenjdk.net/ but you should follow the installer package system that comes with your linux distribution. Download generic YaCy from https://download.yacy.net/yacy_v1.922_20191013_9964.tar.gz , i.e. by Unpack the tarball. Run startYACY.sh inside your decompressed folder. wget https://download.yacy.net/yacy_v1.922_20191013_9964.tar.gz This can be done by running in a terminal: sudo apt-get update sudo dpkg --configure -a sudo apt-get install -y openjdk-8-jre-headless wget https://download.yacy.net/yacy_v1.922_20191013_9964.tar.gz tar xfz yacy_v1.922_20191013_9964.tar.gz cd yacy ./startYACY.sh YaCy is now running on port 8090 on your machine. Open http://localhost:8090 in your web-browser. No password is required when accessing this YaCy instance from localhost. Headless operation can be achieved by setting a password on the host console with bin/passwd.sh <password> . macOS Application https://download.yacy.net/yacy_v1.922_20191013_9964.dmg On Macintosh Install Java 8 from https://adoptopenjdk.net/ . Double-click the downloaded .dmg -file and copy the YaCy app out of the mounted drive into your \"Application\" folder. To run YaCy, just double-click the YaCy app icon in your \"Application\" folder. YaCy is then running on port 8090 on your machine. Open http://localhost:8090 in your web-browser. Any OS with Java 8 Automatic development builds can be obtained from https://download.yacy.net/ . Get the Source Code The source code is inside the generic tarball. It is recommended you clone the Git repository at https://github.com/yacy/yacy_search_server . Compile YaCy yourself You can download and build your own YaCy with these simple steps. You need: jdk-8 installed (Oracle Java 8 or OpenJDK 8) ant git Then run: git clone https://github.com/yacy/yacy_search_server.git cd yacy_search_server ant clean all dist The compiled tarball will then be in the RELEASE folder.","title":"Download"},{"location":"download_installation/#download-and-installation-of-yacy","text":"YaCy is available as packages for Windows, Macintosh and Debian. You can also install YaCy on any other operation system using a tarball. YaCy needs Java 8, OpenJDK 8 is recommended. Improve this doc","title":"Download and Installation of YaCy"},{"location":"download_installation/#please-support-yacy","text":"YaCy is libre software - licensed GPL-2+. Downloads are provided free, with the option to donate:","title":"Please Support YaCy"},{"location":"download_installation/#installation","text":"Installation is very easy on any system. If it looks complex here it's just because we give directions in great detail.","title":"Installation"},{"location":"download_installation/#yacy-for-windows","text":"Install Java 8 from https://adoptopenjdk.net/ . Download YaCy for Windows from https://download.yacy.net/yacy_v1.922_20191014_9966.exe Install YaCy by double-clicking the downloaded installer. When the message \"Windows protected your PC (in Windows 10) appears, klick \"More info\", then \"Run anyway\". When asked \"Do you want to allow this app from an unknown publisher to make changes to your device\", klick \"yes\". Within the YaCy Installer, klick \"Next\", \"I Agree\", \"Next\", \"Install\", \"Finish\". To run YaCy, double-click the YaCy app icon (which may be on your desktop). YaCy is then running on port 8090 on your machine. Open http://localhost:8090 in your web-browser.","title":"YaCy for Windows"},{"location":"download_installation/#yacy-for-linux-and-other-os","text":"You must install Java 8 on your computer if you don't have it already, and start the YaCy start shell script. YaCy can run under a normal user account, no root rights are needed if Java 8 is already installed. Install JDK 8 (Oracle or preferably OpenJDK). You get Java for Linux from https://adoptopenjdk.net/ but you should follow the installer package system that comes with your linux distribution. Download generic YaCy from https://download.yacy.net/yacy_v1.922_20191013_9964.tar.gz , i.e. by Unpack the tarball. Run startYACY.sh inside your decompressed folder. wget https://download.yacy.net/yacy_v1.922_20191013_9964.tar.gz This can be done by running in a terminal: sudo apt-get update sudo dpkg --configure -a sudo apt-get install -y openjdk-8-jre-headless wget https://download.yacy.net/yacy_v1.922_20191013_9964.tar.gz tar xfz yacy_v1.922_20191013_9964.tar.gz cd yacy ./startYACY.sh YaCy is now running on port 8090 on your machine. Open http://localhost:8090 in your web-browser. No password is required when accessing this YaCy instance from localhost. Headless operation can be achieved by setting a password on the host console with bin/passwd.sh <password> .","title":"YaCy for Linux and other OS"},{"location":"download_installation/#macos-application","text":"https://download.yacy.net/yacy_v1.922_20191013_9964.dmg","title":"macOS Application"},{"location":"download_installation/#on-macintosh","text":"Install Java 8 from https://adoptopenjdk.net/ . Double-click the downloaded .dmg -file and copy the YaCy app out of the mounted drive into your \"Application\" folder. To run YaCy, just double-click the YaCy app icon in your \"Application\" folder. YaCy is then running on port 8090 on your machine. Open http://localhost:8090 in your web-browser.","title":"On Macintosh"},{"location":"download_installation/#any-os-with-java-8","text":"Automatic development builds can be obtained from https://download.yacy.net/ .","title":"Any OS with Java 8"},{"location":"download_installation/#get-the-source-code","text":"The source code is inside the generic tarball. It is recommended you clone the Git repository at https://github.com/yacy/yacy_search_server .","title":"Get the Source Code"},{"location":"download_installation/#compile-yacy-yourself","text":"You can download and build your own YaCy with these simple steps. You need: jdk-8 installed (Oracle Java 8 or OpenJDK 8) ant git Then run: git clone https://github.com/yacy/yacy_search_server.git cd yacy_search_server ant clean all dist The compiled tarball will then be in the RELEASE folder.","title":"Compile YaCy yourself"},{"location":"faq/","text":"FAQ - Frequently Asked Questions Improve this doc General What is this? YaCy is a distributed Web Search Engine, based on a peer-to-peer network. What does indexing mean? Indexing means that a web page is separated into the single words on it and to save the URLs to the sites containing them under a reference to the word itself in a database. So searching for a word (or many words) may be easily performed by fetching all URLs \"belonging\" to the search term. What is a DHT? A distributed hash table (DHT) is a class of a decentralized distributed system that provides a lookup service similar to a hash table; (key, value) pairs are stored in a DHT, and any participating node can efficiently retrieve the value associated with a given key. Responsibility for maintaining the mapping from keys to values is distributed among the nodes, in such a way that a change in the set of participants causes a minimal amount of disruption.(source:Wikipedia) What's the meaning of \"to crawl\"? A so-called \"crawler\" fetches a web page and parses out all links on it; this is the first step or \"depth 0\". It continues to get all web pages linked on the first document which is then called \"depth 1\" and does the same respectively for all documents of this step. The crawler is limitable to a specified depth or can even crawl indefinitely and so can crawl the whole \"indexable Web\", including those parts of the indexable web who are censored by commercial search-engines and therefore normally not part of what most people are presented as The visible web. What is a P2P network? Meaning Peer to Peer computing or networking is a distributed application architecture that partitions tasks or work loads between peers. Peers are equally privileged, equipotent participants in the application. They are said to form a peer-to-peer network of nodes. Peers make a portion of their resources, such as processing power, disk storage or network bandwidth, directly available to other network participants, without the need for central coordination by servers or stable hosts. Wikipedia What is a Peer? A peer is a communications endpoint in a computer network. Each peer offers its services and uses the services of other peers. In YaCy, a peer provides web indexing services to other peers on the YaCy search network. What is RWI? RWI is an acronym for Reverse Word Index. This is generated by the indexer from the collected data and stored in the database. Isn't P2P illegal? No. P2P (peer to peer) only describes the technology by which computers exchange data amongst themselves. Past legal disputes have been over what types of data have been exchanged over such networks. Namely, copyrighted material. The P2P file sharing technique itself is legal, despite the fact that it has been used facilitate in the transfer of copyrighted data. The only files shared amongst YaCy peers are indexes of the publicly accessible internet. YaCy in general What are global and local indexes? Once you use the proxy, a local copy of the indexed data is automatically created. You can access the global index only if the program has a connection to the YaCy network. This is effectively the combination of all local indexes. I am not a technican. Can I install YaCy easily and use it to index my own web pages? YaCy is very easy to install. You don't need any special knowledge or additional software; also you don't need to set up an extra database engine. Indexing your own website isn't hard too: Simply crawl it and turn off DHT Distribution and DHT Receive to keep the index of your site on your peer. Can I crawl and index the web with YaCy? Yes. You can start your own crawl and you may also trigger distributed crawling, which means that your own YaCy peer asks other peers to perform specific crawl tasks. You can specify many parameters that focus your crawl to a limited set of web pages. Is there a central server? Does the search engine network need one? No. The YaCy network architecture does not need a central server, and there is none. There are currently four so-called seed-list servers hard-coded into source code due to they are mostly available and have accurate seed list information (see FAQ for details). Search Engines need a lot of terabytes of space, don't they? How much space do I need on my machine? The global index is shared, but not copied to the peers. If you run YaCy, you need an average of the same disc memory amount for the index as you need for the cache. In fact, the global space for the index may reach the space of Terabytes, but not all of that is on your machine! Do I need a fast machine? Search Engines need big server farms, don't they? You don't need a fast machine to run YaCy. You also don't need a lot of space. You can configure the amount of Megabytes that you want to spend for the cache and (currently only indirectly) the index. Any time-critical task is delayed automatically and takes place when you are idle surfing (this works only if you use YaCy as http proxy). How long does a search take? Time for a search depends on many factors. One is, if you perform a local-only search or a global DHT-based search. Another factor comes in place when enable search heuristics. A third and important factor is if you have already searched for it often (short time between two same searches), in this case the search may still be cached. Another factor is if the parameter verify=true, if so the fetched HTML snippet got verified, say fetched again. If you use verify=false this may give you less accurate search results (wrong maybe) but they are way faster. In general YaCy's architecture does not do peer-hopping, it also doesn't have a TTL (time-to-live). It is expected that search results are instantly responded to the requester. This can be done by asking the index-owning peer directly which is in fact possible by using DHT's (distributed hash tables). Because YaCy needs some redundancy to compensate for missing peers, it asks several peers simultaneously. To collect their response, YaCy waits a little time of at most 6 seconds (by default, you can change that). Do I need to set up and run a separate database? No. YaCy contains its own database engine, which does not need any extra set-up or configuration. What does Virgin, Junior, Senior, Principal Status mean? virgin Status Virgin means your peer did not have contact to the network yet. Simply put, you are \"offline\". You can search the local index only. junior Status Junior means your peer has contact to the yacy network but cannot be reached by other peers. One reason could be a firewall or missing router configuration. You can search the local index only. Junior peers can contribute to the network by submitting index files to senior/principal peers without being asked. senior Status Senior means your peer has contact to the yacy network and can be reached by other peers. It is now an access point for index sharing and distribution. You can search the local and global index. This is the best status to use, it supports the network. principal A Principal is a Senior peer that uploaded an additional peer-list to a server. This list supports other peers to get in contact with the existing Yacy network to perform a global search. If you have the possibility to upload a file to an FTP server, you can become a Principal by uploading your peer-list. Why should I run my peer in Senior Mode? Some p2p-based file sharing software assign non-contributing peers very low priority. We think that that this is not always fair since sometimes the operator does not have the choice of opening the firewall or configuring the router accordingly. Our idea of 'information wares' and their exchange can also be applied to junior peers: they must contribute to the global index by submitting their index actively, while senior peers contribute passively. Therefore we don't need to give junior peers low priority: they contribute equally, so they may participate equally. But enough senior peers are needed to make this architecture functional. Since any peer contributes almost equally, either actively or passively, it is not mandatory. However, since any peer can add to the index, but what is added can only be stored on and found through senior peers, you should decide to run in Senior Mode if you can. Even if only 1/10 of the peers which were Junior as of March 2012 become Senior, the network\u2019s capacity will grow considerably. My peer says it runs in 'Junior Mode'. How can I run it in Senior Mode? Open your firewall for port 8090 (or the port you configured) or program your router to forward this port to your computer. Will running YaCy jeopardize my privacy? YaCy respects user privacy. All password- or cookies-protected pages are excluded from indexing. Additionally, pages loaded using GET or POST parameters are not indexed by default. Thus, only publicly accessible, non-password-protected pages will be indexed. For a detailed explanation on the technique: How YaCy protects your privacy wrt to personalized pages. Can other people find-out about my browsing log/history? There's no way to browse the pages that are stored on a peer. A search of the pages is only possible on a precise word. The words are themselves dispatched over the peers thanks to the distributed hash tables (DHT). Then the hash tables of the peers are mixed, which makes retrieving the history of browsing of a certain peer impossible. My YaCy search pages doesn't show! The default address for the YaCy search and administration page is http://localhost:8090. If you are using Internet Explorer, please mind adding http:// before localhost:8090. In case you have changed the default port of YaCy from 8090 to another one, you will have to open the new port in your firewall or router (and maybe close the port 8090 if you don't use it). Another reason could be a bad setting of the proxy, in which case you need to deactivate the proxy for the local pages. Why does YaCy show different results from Google? We expect YaCy to show different results than Google, for several reasons. As long as YaCy has only a few peers working, it cannot compete with Google. Hence the importance of having a great number of YaCy peers working. But even then YaCy will provide different and better results than Google, since it can be adapted to the user's own preferences and is not influenced by commercial aspects. I can not uninstall, because YaCy is still running First check whether YaCy still runs. If it doesn't run, it may not have been shut down properly. Start YaCy again, then uninstall. Alternatively delete the yacy.running file in the yacy/DATA/ directory, then uninstall. How can I help? First of all: run YaCy in senior mode. This helps to enrich the global index and to make YaCy more attractive. If you want to add your own code, you are welcome; but please contact the author first and discuss your idea to see how it may fit into the overall architecture. You can help a lot by simply giving us feedback or telling us about new ideas. You can also help by telling other people about this software. And if you find a bug or you see an uncovered use-case, we welcome your bug-report. Any feed-back is welcome. Technology Something seems not to be working properly ; what should I do? YaCy is still undergoing development, so one should opt for a stable version for use. The latest stable version can be downloaded from the YaCy homepage http://www.yacy.net. If you are experiencing a strange behaviour of YaCy then you should search the Wiki or the forum http://www.yacy-forum.org/ for known issues. If the issue is unknown, then you can ask for help on the forum (and provide the YaCy version, details on the occurrence of the issue, and if possible an excerpt from the log file in order to help fix the bug). How can I index Tor or Freenet pages? The indexing of Tor or Freenet pages is for the moment deliberately avoided in the source code because it is not desired to index these pages at this stage of the development of YaCy. However, the crawling of such sites is planned in the future. Most likely the crawl results will not distributed globally, but will only be available to the local peer. How do I give the index of one peer to another? This actually happens automatically through the DHT distribution of the words. However, there is also the possibility of transferring the entire index to another peer. This can either be done through a so-called Index-Transfer (link needs update) or a index-Import (link needs update). Will already-indexed pages (i.e. indexed and index-exchanged) automatically be reindexed after a few days/years? Unfortunately no. However, there is the possibility for a chronological \"recrawl\" to be executed for a URL (or an entire website if desired). Learn more about this feature under \"Index Control\" -> \"Index Creation.\" Are DHT entries unique in a search network or can URLs also appear twice or three times? URLs are analyzed more than once so that a peer delayed does not lose his part in the search index. As for the indexes they are stored redundantly on multiple peers. How can I change the Connection Timeout value? This can be done on the configuration page \"Admin Console\" -> \"Advanced behavior\" http://localhost:8090/ConfigProperties_p.html. Just search for the line client-timeout and change the value there. The timeout is in milliseconds. Do not forget to restart YaCy after the change. Alternatively, another way to do this is through the configuration file httpProxy.conf in DATA/SETTINGS. If this type of configuration is to be performed then YaCy must be stopped before. I can not log in YaCy anymore as I forgot my password. How do I reset my password? If you have lost your password, you can reset it (or choose a new one). There are two methods: Password reset while YaCy is running This is the most convenient way. You don't need to shut down YaCy: Use a command line terminal and log in to the user account running YaCy then execute <yacy-app>/bin/passwd.sh <new-password> This changes only the admin account password for the account named 'admin'. Password reset while YaCy is shut down: edit the file DATA/SETTINGS/yacy.conf: remove the entry serverAccountBase64MD5 remove the entry adminAccount (if any) choose a new password by setting the entry serverAccount to \\<account>:\\<password>, for example serverAccount=admin:mysecretpassword The next time you start YaCy the account/password combination will be read, encrypted and then deleted from yacy.conf, so that it will not be available in plain text anywhere anymore. Then you will be able to log again into YaCy with the account/password you entered in the yacy.conf file, or set another password if you didn't set a combination. Disk space How can I limit the size of single files to be downloaded? The maximum file size can be set under Advanced settings -> Crawler settings. Maximum sizes can be specified for HTTP and FTP. The file size is in bytes. A converter can be found at Bit and Byte (bits and bytes) (broken link) How many links/words and how much disk space can a YaCy instance manage? The number of storable links/words is theoretically not limited, but it becomes actually limited following the slowdown of the indexing process with the increase of the links/words number. There are users with more than 10 million Web pages indexed in their YaCy instance. Also, the necessary space for the index of a web page depends on the size and nature of the document. With 10 million web pages indexed, an index size of 20GB is not uncommon. Can I limit the size of the indexes on my hard-drive? For the moment not directly. Automatically limiting that size would mean having to delete stored indexes, which is not suitable. You can set two minimums of free disk space at /Performance_p.html: one for the crawls, and the other for DHT-in. The number for crawls seems to have to be equal or bigger than the number for DHT-in. Note that, with DHT-in disabled, global searching using the peer's UI is disabled. Also proxy/crawling privacy might suffer. You can also just disable \u201cIndex Receive\u201d at /ConfigNetwork_p.html, so that your index is only augmented through crawling (over which you have some control). For a very indirect additional limit, you can change the Index Reference Size at /IndexControlRWIs_p.html.","title":"FAQ"},{"location":"faq/#faq-frequently-asked-questions","text":"Improve this doc","title":"FAQ - Frequently Asked Questions"},{"location":"faq/#general","text":"","title":"General"},{"location":"faq/#what-is-this","text":"YaCy is a distributed Web Search Engine, based on a peer-to-peer network.","title":"What is this?"},{"location":"faq/#what-does-indexing-mean","text":"Indexing means that a web page is separated into the single words on it and to save the URLs to the sites containing them under a reference to the word itself in a database. So searching for a word (or many words) may be easily performed by fetching all URLs \"belonging\" to the search term.","title":"What does indexing mean?"},{"location":"faq/#what-is-a-dht","text":"A distributed hash table (DHT) is a class of a decentralized distributed system that provides a lookup service similar to a hash table; (key, value) pairs are stored in a DHT, and any participating node can efficiently retrieve the value associated with a given key. Responsibility for maintaining the mapping from keys to values is distributed among the nodes, in such a way that a change in the set of participants causes a minimal amount of disruption.(source:Wikipedia)","title":"What is a DHT?"},{"location":"faq/#whats-the-meaning-of-to-crawl","text":"A so-called \"crawler\" fetches a web page and parses out all links on it; this is the first step or \"depth 0\". It continues to get all web pages linked on the first document which is then called \"depth 1\" and does the same respectively for all documents of this step. The crawler is limitable to a specified depth or can even crawl indefinitely and so can crawl the whole \"indexable Web\", including those parts of the indexable web who are censored by commercial search-engines and therefore normally not part of what most people are presented as The visible web.","title":"What's the meaning of \"to crawl\"?"},{"location":"faq/#what-is-a-p2p-network","text":"Meaning Peer to Peer computing or networking is a distributed application architecture that partitions tasks or work loads between peers. Peers are equally privileged, equipotent participants in the application. They are said to form a peer-to-peer network of nodes. Peers make a portion of their resources, such as processing power, disk storage or network bandwidth, directly available to other network participants, without the need for central coordination by servers or stable hosts. Wikipedia","title":"What is a P2P network?"},{"location":"faq/#what-is-a-peer","text":"A peer is a communications endpoint in a computer network. Each peer offers its services and uses the services of other peers. In YaCy, a peer provides web indexing services to other peers on the YaCy search network.","title":"What is a Peer?"},{"location":"faq/#what-is-rwi","text":"RWI is an acronym for Reverse Word Index. This is generated by the indexer from the collected data and stored in the database.","title":"What is RWI?"},{"location":"faq/#isnt-p2p-illegal","text":"No. P2P (peer to peer) only describes the technology by which computers exchange data amongst themselves. Past legal disputes have been over what types of data have been exchanged over such networks. Namely, copyrighted material. The P2P file sharing technique itself is legal, despite the fact that it has been used facilitate in the transfer of copyrighted data. The only files shared amongst YaCy peers are indexes of the publicly accessible internet.","title":"Isn't P2P illegal?"},{"location":"faq/#yacy-in-general","text":"","title":"YaCy in general"},{"location":"faq/#what-are-global-and-local-indexes","text":"Once you use the proxy, a local copy of the indexed data is automatically created. You can access the global index only if the program has a connection to the YaCy network. This is effectively the combination of all local indexes.","title":"What are global and local indexes?"},{"location":"faq/#i-am-not-a-technican-can-i-install-yacy-easily-and-use-it-to-index-my-own-web-pages","text":"YaCy is very easy to install. You don't need any special knowledge or additional software; also you don't need to set up an extra database engine. Indexing your own website isn't hard too: Simply crawl it and turn off DHT Distribution and DHT Receive to keep the index of your site on your peer.","title":"I am not a technican. Can I install YaCy easily and use it to index my own web pages?"},{"location":"faq/#can-i-crawl-and-index-the-web-with-yacy","text":"Yes. You can start your own crawl and you may also trigger distributed crawling, which means that your own YaCy peer asks other peers to perform specific crawl tasks. You can specify many parameters that focus your crawl to a limited set of web pages.","title":"Can I crawl and index the web with YaCy?"},{"location":"faq/#is-there-a-central-server-does-the-search-engine-network-need-one","text":"No. The YaCy network architecture does not need a central server, and there is none. There are currently four so-called seed-list servers hard-coded into source code due to they are mostly available and have accurate seed list information (see FAQ for details).","title":"Is there a central server? Does the search engine network need one?"},{"location":"faq/#search-engines-need-a-lot-of-terabytes-of-space-dont-they-how-much-space-do-i-need-on-my-machine","text":"The global index is shared, but not copied to the peers. If you run YaCy, you need an average of the same disc memory amount for the index as you need for the cache. In fact, the global space for the index may reach the space of Terabytes, but not all of that is on your machine!","title":"Search Engines need a lot of terabytes of space, don't they? How much space do I need on my machine?"},{"location":"faq/#do-i-need-a-fast-machine-search-engines-need-big-server-farms-dont-they","text":"You don't need a fast machine to run YaCy. You also don't need a lot of space. You can configure the amount of Megabytes that you want to spend for the cache and (currently only indirectly) the index. Any time-critical task is delayed automatically and takes place when you are idle surfing (this works only if you use YaCy as http proxy).","title":"Do I need a fast machine? Search Engines need big server farms, don't they?"},{"location":"faq/#how-long-does-a-search-take","text":"Time for a search depends on many factors. One is, if you perform a local-only search or a global DHT-based search. Another factor comes in place when enable search heuristics. A third and important factor is if you have already searched for it often (short time between two same searches), in this case the search may still be cached. Another factor is if the parameter verify=true, if so the fetched HTML snippet got verified, say fetched again. If you use verify=false this may give you less accurate search results (wrong maybe) but they are way faster. In general YaCy's architecture does not do peer-hopping, it also doesn't have a TTL (time-to-live). It is expected that search results are instantly responded to the requester. This can be done by asking the index-owning peer directly which is in fact possible by using DHT's (distributed hash tables). Because YaCy needs some redundancy to compensate for missing peers, it asks several peers simultaneously. To collect their response, YaCy waits a little time of at most 6 seconds (by default, you can change that).","title":"How long does a search take?"},{"location":"faq/#do-i-need-to-set-up-and-run-a-separate-database","text":"No. YaCy contains its own database engine, which does not need any extra set-up or configuration.","title":"Do I need to set up and run a separate database?"},{"location":"faq/#what-does-virgin-junior-senior-principal-status-mean","text":"","title":"What does Virgin, Junior, Senior, Principal Status mean?"},{"location":"faq/#virgin","text":"Status Virgin means your peer did not have contact to the network yet. Simply put, you are \"offline\". You can search the local index only.","title":"virgin"},{"location":"faq/#junior","text":"Status Junior means your peer has contact to the yacy network but cannot be reached by other peers. One reason could be a firewall or missing router configuration. You can search the local index only. Junior peers can contribute to the network by submitting index files to senior/principal peers without being asked.","title":"junior"},{"location":"faq/#senior","text":"Status Senior means your peer has contact to the yacy network and can be reached by other peers. It is now an access point for index sharing and distribution. You can search the local and global index. This is the best status to use, it supports the network.","title":"senior"},{"location":"faq/#principal","text":"A Principal is a Senior peer that uploaded an additional peer-list to a server. This list supports other peers to get in contact with the existing Yacy network to perform a global search. If you have the possibility to upload a file to an FTP server, you can become a Principal by uploading your peer-list.","title":"principal"},{"location":"faq/#why-should-i-run-my-peer-in-senior-mode","text":"Some p2p-based file sharing software assign non-contributing peers very low priority. We think that that this is not always fair since sometimes the operator does not have the choice of opening the firewall or configuring the router accordingly. Our idea of 'information wares' and their exchange can also be applied to junior peers: they must contribute to the global index by submitting their index actively, while senior peers contribute passively. Therefore we don't need to give junior peers low priority: they contribute equally, so they may participate equally. But enough senior peers are needed to make this architecture functional. Since any peer contributes almost equally, either actively or passively, it is not mandatory. However, since any peer can add to the index, but what is added can only be stored on and found through senior peers, you should decide to run in Senior Mode if you can. Even if only 1/10 of the peers which were Junior as of March 2012 become Senior, the network\u2019s capacity will grow considerably.","title":"Why should I run my peer in Senior Mode?"},{"location":"faq/#my-peer-says-it-runs-in-junior-mode-how-can-i-run-it-in-senior-mode","text":"Open your firewall for port 8090 (or the port you configured) or program your router to forward this port to your computer.","title":"My peer says it runs in 'Junior Mode'. How can I run it in Senior Mode?"},{"location":"faq/#will-running-yacy-jeopardize-my-privacy","text":"YaCy respects user privacy. All password- or cookies-protected pages are excluded from indexing. Additionally, pages loaded using GET or POST parameters are not indexed by default. Thus, only publicly accessible, non-password-protected pages will be indexed. For a detailed explanation on the technique: How YaCy protects your privacy wrt to personalized pages.","title":"Will running YaCy jeopardize my privacy?"},{"location":"faq/#can-other-people-find-out-about-my-browsing-loghistory","text":"There's no way to browse the pages that are stored on a peer. A search of the pages is only possible on a precise word. The words are themselves dispatched over the peers thanks to the distributed hash tables (DHT). Then the hash tables of the peers are mixed, which makes retrieving the history of browsing of a certain peer impossible.","title":"Can other people find-out about my browsing log/history?"},{"location":"faq/#my-yacy-search-pages-doesnt-show","text":"The default address for the YaCy search and administration page is http://localhost:8090. If you are using Internet Explorer, please mind adding http:// before localhost:8090. In case you have changed the default port of YaCy from 8090 to another one, you will have to open the new port in your firewall or router (and maybe close the port 8090 if you don't use it). Another reason could be a bad setting of the proxy, in which case you need to deactivate the proxy for the local pages.","title":"My YaCy search pages doesn't show!"},{"location":"faq/#why-does-yacy-show-different-results-from-google","text":"We expect YaCy to show different results than Google, for several reasons. As long as YaCy has only a few peers working, it cannot compete with Google. Hence the importance of having a great number of YaCy peers working. But even then YaCy will provide different and better results than Google, since it can be adapted to the user's own preferences and is not influenced by commercial aspects.","title":"Why does YaCy show different results from Google?"},{"location":"faq/#i-can-not-uninstall-because-yacy-is-still-running","text":"First check whether YaCy still runs. If it doesn't run, it may not have been shut down properly. Start YaCy again, then uninstall. Alternatively delete the yacy.running file in the yacy/DATA/ directory, then uninstall.","title":"I can not uninstall, because YaCy is still running"},{"location":"faq/#how-can-i-help","text":"First of all: run YaCy in senior mode. This helps to enrich the global index and to make YaCy more attractive. If you want to add your own code, you are welcome; but please contact the author first and discuss your idea to see how it may fit into the overall architecture. You can help a lot by simply giving us feedback or telling us about new ideas. You can also help by telling other people about this software. And if you find a bug or you see an uncovered use-case, we welcome your bug-report. Any feed-back is welcome.","title":"How can I help?"},{"location":"faq/#technology","text":"","title":"Technology"},{"location":"faq/#something-seems-not-to-be-working-properly-what-should-i-do","text":"YaCy is still undergoing development, so one should opt for a stable version for use. The latest stable version can be downloaded from the YaCy homepage http://www.yacy.net. If you are experiencing a strange behaviour of YaCy then you should search the Wiki or the forum http://www.yacy-forum.org/ for known issues. If the issue is unknown, then you can ask for help on the forum (and provide the YaCy version, details on the occurrence of the issue, and if possible an excerpt from the log file in order to help fix the bug).","title":"Something seems not to be working properly ; what should I do?"},{"location":"faq/#how-can-i-index-tor-or-freenet-pages","text":"The indexing of Tor or Freenet pages is for the moment deliberately avoided in the source code because it is not desired to index these pages at this stage of the development of YaCy. However, the crawling of such sites is planned in the future. Most likely the crawl results will not distributed globally, but will only be available to the local peer.","title":"How can I index Tor or Freenet pages?"},{"location":"faq/#how-do-i-give-the-index-of-one-peer-to-another","text":"This actually happens automatically through the DHT distribution of the words. However, there is also the possibility of transferring the entire index to another peer. This can either be done through a so-called Index-Transfer (link needs update) or a index-Import (link needs update).","title":"How do I give the index of one peer to another?"},{"location":"faq/#will-already-indexed-pages-ie-indexed-and-index-exchanged-automatically-be-reindexed-after-a-few-daysyears","text":"Unfortunately no. However, there is the possibility for a chronological \"recrawl\" to be executed for a URL (or an entire website if desired). Learn more about this feature under \"Index Control\" -> \"Index Creation.\"","title":"Will already-indexed pages (i.e. indexed and index-exchanged) automatically be reindexed after a few days/years?"},{"location":"faq/#are-dht-entries-unique-in-a-search-network-or-can-urls-also-appear-twice-or-three-times","text":"URLs are analyzed more than once so that a peer delayed does not lose his part in the search index. As for the indexes they are stored redundantly on multiple peers.","title":"Are DHT entries unique in a search network or can URLs also appear twice or three times?"},{"location":"faq/#how-can-i-change-the-connection-timeout-value","text":"This can be done on the configuration page \"Admin Console\" -> \"Advanced behavior\" http://localhost:8090/ConfigProperties_p.html. Just search for the line client-timeout and change the value there. The timeout is in milliseconds. Do not forget to restart YaCy after the change. Alternatively, another way to do this is through the configuration file httpProxy.conf in DATA/SETTINGS. If this type of configuration is to be performed then YaCy must be stopped before.","title":"How can I change the Connection Timeout value?"},{"location":"faq/#i-can-not-log-in-yacy-anymore-as-i-forgot-my-password-how-do-i-reset-my-password","text":"If you have lost your password, you can reset it (or choose a new one). There are two methods:","title":"I can not log in YaCy anymore as I forgot my password. How do I reset my password?"},{"location":"faq/#password-reset-while-yacy-is-running","text":"This is the most convenient way. You don't need to shut down YaCy: Use a command line terminal and log in to the user account running YaCy then execute <yacy-app>/bin/passwd.sh <new-password> This changes only the admin account password for the account named 'admin'. Password reset while YaCy is shut down: edit the file DATA/SETTINGS/yacy.conf: remove the entry serverAccountBase64MD5 remove the entry adminAccount (if any) choose a new password by setting the entry serverAccount to \\<account>:\\<password>, for example serverAccount=admin:mysecretpassword The next time you start YaCy the account/password combination will be read, encrypted and then deleted from yacy.conf, so that it will not be available in plain text anywhere anymore. Then you will be able to log again into YaCy with the account/password you entered in the yacy.conf file, or set another password if you didn't set a combination.","title":"Password reset while YaCy is running"},{"location":"faq/#disk-space","text":"","title":"Disk space"},{"location":"faq/#how-can-i-limit-the-size-of-single-files-to-be-downloaded","text":"The maximum file size can be set under Advanced settings -> Crawler settings. Maximum sizes can be specified for HTTP and FTP. The file size is in bytes. A converter can be found at Bit and Byte (bits and bytes) (broken link)","title":"How can I limit the size of single files to be downloaded?"},{"location":"faq/#how-many-linkswords-and-how-much-disk-space-can-a-yacy-instance-manage","text":"The number of storable links/words is theoretically not limited, but it becomes actually limited following the slowdown of the indexing process with the increase of the links/words number. There are users with more than 10 million Web pages indexed in their YaCy instance. Also, the necessary space for the index of a web page depends on the size and nature of the document. With 10 million web pages indexed, an index size of 20GB is not uncommon.","title":"How many links/words and how much disk space can a YaCy instance manage?"},{"location":"faq/#can-i-limit-the-size-of-the-indexes-on-my-hard-drive","text":"For the moment not directly. Automatically limiting that size would mean having to delete stored indexes, which is not suitable. You can set two minimums of free disk space at /Performance_p.html: one for the crawls, and the other for DHT-in. The number for crawls seems to have to be equal or bigger than the number for DHT-in. Note that, with DHT-in disabled, global searching using the peer's UI is disabled. Also proxy/crawling privacy might suffer. You can also just disable \u201cIndex Receive\u201d at /ConfigNetwork_p.html, so that your index is only augmented through crawling (over which you have some control). For a very indirect additional limit, you can change the Index Reference Size at /IndexControlRWIs_p.html.","title":"Can I limit the size of the indexes on my hard-drive?"},{"location":"impressum/","text":"YaCy is a community development of more than 50 people. The project was started by Michael Christen in 2004. For professional consulting, please use this contact:","title":"Impressum"},{"location":"manual/","text":"","title":"Manual"},{"location":"network/","text":"The YaCy Network Hash Name PPM QPH Uptime $(document).ready(function() { $('#networktable').DataTable( { \"paging\": false, \"ordering\": false, \"info\": false, \"searching\": false, \"ajax\": { \"url\": \"https://yacy.searchlab.eu/Network.json?page=1&maxCount=1000\", \"dataSrc\": \"peers\" }, \"columns\": [ { \"data\": \"hash\", \"defaultContent\": \"\" }, { \"data\": \"fullname\", \"defaultContent\": \"\" }, { \"data\": \"ppm\", \"defaultContent\": \"\" }, { \"data\": \"qph\", \"defaultContent\": \"\" }, { \"data\": \"uptime\", \"defaultContent\": \"\" } ] } ); } );","title":"The YaCy Network"},{"location":"network/#the-yacy-network","text":"Hash Name PPM QPH Uptime $(document).ready(function() { $('#networktable').DataTable( { \"paging\": false, \"ordering\": false, \"info\": false, \"searching\": false, \"ajax\": { \"url\": \"https://yacy.searchlab.eu/Network.json?page=1&maxCount=1000\", \"dataSrc\": \"peers\" }, \"columns\": [ { \"data\": \"hash\", \"defaultContent\": \"\" }, { \"data\": \"fullname\", \"defaultContent\": \"\" }, { \"data\": \"ppm\", \"defaultContent\": \"\" }, { \"data\": \"qph\", \"defaultContent\": \"\" }, { \"data\": \"uptime\", \"defaultContent\": \"\" } ] } ); } );","title":"The YaCy Network"},{"location":"privacy/","text":"","title":"Privacy"},{"location":"api/crawler/","text":"YaCy Crawler API A web crawl is stared using either the web page at /CrawlStartSite_p.html or /CrawlStartExpert_p.html, however, both web pages call the servlet at /Crawler_p.html. The /Crawler_p.html can be called directly to show monitoring information but it is also the API access point to start a crawl using a direct call to the url as shown below: http://localhost:8090/Crawler_p.html?crawlingDomMaxPages=10000&range=wide&intention=&sitemapURL=&crawlingQ=on&crawlingMode=url&crawlingURL=http://vip.asus.com/forum/default.aspx%3FSLanguage%3Den-us&crawlingFile=&mustnotmatch=&crawlingFile%24file=&crawlingstart=Neuen%20Crawl%20starten&mustmatch=.*&createBookmark=on&bookmarkFolder=/crawlStart&xsstopw=on&indexMedia=on&crawlingIfOlderUnit=hour&cachePolicy=iffresh&indexText=on&crawlingIfOlderCheck=on&bookmarkTitle=&crawlingDomFilterDepth=1&crawlingDomFilterCheck=on&crawlingIfOlderNumber=1&crawlingDepth=4 The parameters used here are explained below in detail. Each YaCy crawl job has its own profile to store information to ensure proper handling of crawled URLs. It is created at crawl start, will be set as terminated if a crawl is considered to be finished, and may also be edited or deleted while the crawl is running. To start a new crawl and create its profile following parameters are needed crawlingstart = (no value needed) - this key must be present to trigger a crawl start crawlingMode = Possible values: 'url', 'sitemap', 'sitelist', 'file'. The crawler can be started with different modes: 'url': start from a given url which is the root of a crawl tree. The url is given in 'crawlingURL'. If this url is a http-link, then the Crawl will subsequently load all linked documents from http until a given depth is reached. If this url is a smb- or ftp-link, then the given resource will be listed completely using a special listing process. 'sitemap': use a sitemap to retrieve all files that are listed in the sitemap. The sitemap-URL is given in 'sitemapURL' 'sitelist': use a list of crawl start URLs. This is like starting with one url, but using several of them. The list of urls is retrieved by loading the url given in 'crawlingURL'. Each of the urls in that file is then used to start it's individual crawl. This makes sense if the 'range' attribute contains the value 'domain' or 'subpath' which creates an individual must-match pattern for each of the urls in the sitelist. 'file': use a file in the local file system to provide a start document. The crawl will then start like with a root url but the file itself will not be placed to the index, only the documents which are linked in that document. crawlingURL = The crawl start url. This value must be present in all cases of the crawlingMode options. The crawler's double-check does not check this start url which means that in the index existing start-URLs are always loaded (but can be loaded from the cache, see the cachePolicy option for that. sitemapURL = Only to be defined if 'crawlingMode' = 'sitemap'. This is an url which is typically linked within a robots.txt file. The sitemapURL must point to a resource which is formed as described in http://www.sitemaps.org crawlingFile = Only to be defined if 'crawlingMode' = 'file'. This is a path to a file in the local file system. The content of the file is parsed and all urls inside that document are roots for crawl starts as defined in this crawl request. crawlingDepth = This defines how often the Crawler will follow links embedded in websites. A minimum of 0 is recommended and means that the page set as crawling URL, sitemap orfile will be added to the index, but no linked content is indexed. 2-4 is good for normal indexing. Be careful with the depth, consider a branching factor of average 20; A crawleing depth of 8 would index 25.600.000.000 pages, maybe this is the whole WWW. crawlingDepthExtension = This is a regular expression that can be used to extend the crawling depth to infinity. That means, if this pattern matches with the URL, the crawling depth is not a limitation any more on the ongoing crawl. Default is an empty String, which is a never-match regular expression. range = Possible values are 'domain', 'subpath' or not set, which means 'wide'. Default is 'wide'. If this value is set to 'domain' or 'subpath', the 'mustmatch' parameter is set automatically and overrides given values in that field. 'domain' creates a 'mustmatch' value which restricts the crawl to pages on the same domain of the start-url; 'subpath' creates a 'mustmatch' value which restricts the crawl to pages within the same subpath of the start-url. mustmatch = The filter is a regular expression that must match with the URLs which are used to be crawled; default is 'catch all'. Example: to allow only urls that contain the word 'science', the filter is set to '. science. '. An automatic domain-restriction can be used to fully crawl a single domain. mustnotmatch = This filter must not match with the URL to allow that the page is accepted for crawling. The empty string is a never-match filter which should do well for most cases. ipMustmatch = The filter is a regular expression that must match with the IP of the host to be crawled; default is 'catch all'. ipMustnotmatch = This filter must not match with the IP of the host to allow that the page is accepted for crawling. The empty string is a never-match filter which should do well for most cases. indexmustmatch = This filter can be used to filter documents to be indexed. Even if a document was loaded by the crawler because the mustmatch filter allows that, this filter can be used to restrict documents to be indexed. Default is catch-all. indexmustnotmatch = This filter must not match to allow that the page is accepted for indexong. Default is an empty string, which is a never-match to exclude no documents from indexing. deleteold = if 'range' is either 'domain' or 'subpath' or the mustmatch-value is not catchall, this option can be used to delete all urls for given start-hosts. Possible values for 'deleteold' are 'off' (do not delete), 'on' (delete all documents for the start host(s), or 'age'. If 'age' is used, the values 'deleteIfOlderNumber' and 'deleteIfOlderUnit' must be set and this defines the limit for the deletion: if the load date of a stored document is older than the given time, then it is deleted. deleteIfOlderNumber = If 'deleteold' is 'age', then this must be set with a numeric value. The unit is given in field 'deleteIfOlderUnit'. deleteIfOlderUnit = Possible values are 'year', 'month', 'day', 'hour' recrawl = value is either 'nodoubles' or 'reload'. This value is a switch which activates the usage of 'reloadIfOlderNumber', and 'reloadIfOlderUnit' values. If 'nodoubles' is selected then every url that the crawler discovers and is already stored in the index is rejected and not loaded again. If 'reload' is selected then 'reloadIfOlderNumber' and 'reloadIfOlderUnit' is a limit for the double-check: if the load date of a new page url is older than the given time, then it is considered as stale and reloaded for indexing. If it is newer then it is considered as fresh and therefore as 'double' in the double-chech. reloadIfOlderNumber = If 'recrawl' is 'reload', then this must be set with a numeric value. The unit is given in field 'crawlingIfOlderUnit'. reloadIfOlderUnit = Possible values are 'year', 'month', 'day', 'hour' crawlingDomMaxCheck = The maxmimum number of pages that are fetched and indexed from a single domain can be limited with this option. If combined with the 'Auto-Dom-Filter' the limit is applied to all the domains within the given depth. Domains outside the given depth are then sorted-out anyway. crawlingDomMaxPages = Integer values are allowed. If this value is given, then the maximum number of pages per domain is restricted to this given number. crawlingQ = value is either 'on' or 'off'; default is 'off'. A questionmark is usually a hint for a dynamic page. URLs pointing to dynamic content should usually not be crawled. However, there are sometimes web pages with static content that is accessed with URLs containing question marks. directDocByURL = value is either 'on' or 'off'; default is 'off'. If this is 'on', then all documents which are linked from loaded pages are indexed with their URL only and without any metadata. This applies also if these documents are outside of the crawl depth or do not match with regular expressions. storeHTCache = value is either 'on' or 'off'; default is 'off'. If 'on', all downloaded content is stored in a built-in cache, the HTCache. cachePolicy = The caching policy states when to read from the HTCache during crawling: no cache: never use the cache, all content from fresh internet source; if fresh: use the cache if the cache exists and is fresh using the proxy-fresh rules; if exist: use the cache if the cache exist. Do no check freshness. Othervise use online source; cache only: never go online, use all content from cache. If no cache exist, treat content as unavailable indexText = value is either 'on' or 'off'; default is 'off'. If this is 'on' then the fulltext is indexed. If 'off', only the metadata of the pages or media according to 'indexMedia' is recorded and indexed. indexMedia = value is either 'on' or 'off'; default is 'off'. If this is 'on' then the metadta (if available) of media content. If 'off', only the metadata of the pages or fulltext according to 'indexText' is recorded and indexed. crawlOrder = value is either 'on' or 'off'; default is 'off'. If 'on' the crawler will contact other peers and use them as remote indexers for your crawl. If crwling results are needed locally, this switch should be set to false. Only senior and principal peers can initiate or receive remote crawls. A YaCyNews message will be created to inform all peers about a global crawl, so they can omit starting a crawl with the same start point. intention = This is a text message that is posted along with the crawlOrder option to inform other YaCy peers about a distributed crawl. If 'crawlOrder' is 'off', this value is not used. xsstopw = This can be useful to circumvent that extremely common words are added to the database, i.e. \"the\", \"he\", \"she\", \"it\"... To exclude all words given in the file yacy.stopwords from indexing, this hast to be set true. collection = The name of a collection or a comma-separated list of collections. This collections can be used to separate search results into different subsets which is used with the GSA search interface using the 'site' parameter in the search request. agentName = The name of a user agent that is used for crawling. Possible values are: 'YaCy Internet (cautious)', 'YaCy Intranet (greedy)', 'Googlebot', 'Random Browser' or 'Custom Agent'. If 'Custom Agent' is used, then the user agent credentials can be set in yacy.conf with the attributes 'crawler.userAgent.name' (i.e. 'yacybot'), 'crawler.userAgent.string' (i.e. 'yacybot ($$SYSTEM$$) http://yacy.net/bot.html'), 'crawler.userAgent.minimumdelta' (i.e. '500') and 'crawler.userAgent.clienttimeout' (i.e. '10000'). The 'crawler.userAgent.name' is the token which is selected in robots.txt files for robots rules. To use these values, there must not appear the word 'yacy' anywhere in these attributes (even if they are given here in the example)","title":"YaCy Crawler API"},{"location":"api/crawler/#yacy-crawler-api","text":"A web crawl is stared using either the web page at /CrawlStartSite_p.html or /CrawlStartExpert_p.html, however, both web pages call the servlet at /Crawler_p.html. The /Crawler_p.html can be called directly to show monitoring information but it is also the API access point to start a crawl using a direct call to the url as shown below: http://localhost:8090/Crawler_p.html?crawlingDomMaxPages=10000&range=wide&intention=&sitemapURL=&crawlingQ=on&crawlingMode=url&crawlingURL=http://vip.asus.com/forum/default.aspx%3FSLanguage%3Den-us&crawlingFile=&mustnotmatch=&crawlingFile%24file=&crawlingstart=Neuen%20Crawl%20starten&mustmatch=.*&createBookmark=on&bookmarkFolder=/crawlStart&xsstopw=on&indexMedia=on&crawlingIfOlderUnit=hour&cachePolicy=iffresh&indexText=on&crawlingIfOlderCheck=on&bookmarkTitle=&crawlingDomFilterDepth=1&crawlingDomFilterCheck=on&crawlingIfOlderNumber=1&crawlingDepth=4 The parameters used here are explained below in detail. Each YaCy crawl job has its own profile to store information to ensure proper handling of crawled URLs. It is created at crawl start, will be set as terminated if a crawl is considered to be finished, and may also be edited or deleted while the crawl is running. To start a new crawl and create its profile following parameters are needed crawlingstart = (no value needed) - this key must be present to trigger a crawl start crawlingMode = Possible values: 'url', 'sitemap', 'sitelist', 'file'. The crawler can be started with different modes: 'url': start from a given url which is the root of a crawl tree. The url is given in 'crawlingURL'. If this url is a http-link, then the Crawl will subsequently load all linked documents from http until a given depth is reached. If this url is a smb- or ftp-link, then the given resource will be listed completely using a special listing process. 'sitemap': use a sitemap to retrieve all files that are listed in the sitemap. The sitemap-URL is given in 'sitemapURL' 'sitelist': use a list of crawl start URLs. This is like starting with one url, but using several of them. The list of urls is retrieved by loading the url given in 'crawlingURL'. Each of the urls in that file is then used to start it's individual crawl. This makes sense if the 'range' attribute contains the value 'domain' or 'subpath' which creates an individual must-match pattern for each of the urls in the sitelist. 'file': use a file in the local file system to provide a start document. The crawl will then start like with a root url but the file itself will not be placed to the index, only the documents which are linked in that document. crawlingURL = The crawl start url. This value must be present in all cases of the crawlingMode options. The crawler's double-check does not check this start url which means that in the index existing start-URLs are always loaded (but can be loaded from the cache, see the cachePolicy option for that. sitemapURL = Only to be defined if 'crawlingMode' = 'sitemap'. This is an url which is typically linked within a robots.txt file. The sitemapURL must point to a resource which is formed as described in http://www.sitemaps.org crawlingFile = Only to be defined if 'crawlingMode' = 'file'. This is a path to a file in the local file system. The content of the file is parsed and all urls inside that document are roots for crawl starts as defined in this crawl request. crawlingDepth = This defines how often the Crawler will follow links embedded in websites. A minimum of 0 is recommended and means that the page set as crawling URL, sitemap orfile will be added to the index, but no linked content is indexed. 2-4 is good for normal indexing. Be careful with the depth, consider a branching factor of average 20; A crawleing depth of 8 would index 25.600.000.000 pages, maybe this is the whole WWW. crawlingDepthExtension = This is a regular expression that can be used to extend the crawling depth to infinity. That means, if this pattern matches with the URL, the crawling depth is not a limitation any more on the ongoing crawl. Default is an empty String, which is a never-match regular expression. range = Possible values are 'domain', 'subpath' or not set, which means 'wide'. Default is 'wide'. If this value is set to 'domain' or 'subpath', the 'mustmatch' parameter is set automatically and overrides given values in that field. 'domain' creates a 'mustmatch' value which restricts the crawl to pages on the same domain of the start-url; 'subpath' creates a 'mustmatch' value which restricts the crawl to pages within the same subpath of the start-url. mustmatch = The filter is a regular expression that must match with the URLs which are used to be crawled; default is 'catch all'. Example: to allow only urls that contain the word 'science', the filter is set to '. science. '. An automatic domain-restriction can be used to fully crawl a single domain. mustnotmatch = This filter must not match with the URL to allow that the page is accepted for crawling. The empty string is a never-match filter which should do well for most cases. ipMustmatch = The filter is a regular expression that must match with the IP of the host to be crawled; default is 'catch all'. ipMustnotmatch = This filter must not match with the IP of the host to allow that the page is accepted for crawling. The empty string is a never-match filter which should do well for most cases. indexmustmatch = This filter can be used to filter documents to be indexed. Even if a document was loaded by the crawler because the mustmatch filter allows that, this filter can be used to restrict documents to be indexed. Default is catch-all. indexmustnotmatch = This filter must not match to allow that the page is accepted for indexong. Default is an empty string, which is a never-match to exclude no documents from indexing. deleteold = if 'range' is either 'domain' or 'subpath' or the mustmatch-value is not catchall, this option can be used to delete all urls for given start-hosts. Possible values for 'deleteold' are 'off' (do not delete), 'on' (delete all documents for the start host(s), or 'age'. If 'age' is used, the values 'deleteIfOlderNumber' and 'deleteIfOlderUnit' must be set and this defines the limit for the deletion: if the load date of a stored document is older than the given time, then it is deleted. deleteIfOlderNumber = If 'deleteold' is 'age', then this must be set with a numeric value. The unit is given in field 'deleteIfOlderUnit'. deleteIfOlderUnit = Possible values are 'year', 'month', 'day', 'hour' recrawl = value is either 'nodoubles' or 'reload'. This value is a switch which activates the usage of 'reloadIfOlderNumber', and 'reloadIfOlderUnit' values. If 'nodoubles' is selected then every url that the crawler discovers and is already stored in the index is rejected and not loaded again. If 'reload' is selected then 'reloadIfOlderNumber' and 'reloadIfOlderUnit' is a limit for the double-check: if the load date of a new page url is older than the given time, then it is considered as stale and reloaded for indexing. If it is newer then it is considered as fresh and therefore as 'double' in the double-chech. reloadIfOlderNumber = If 'recrawl' is 'reload', then this must be set with a numeric value. The unit is given in field 'crawlingIfOlderUnit'. reloadIfOlderUnit = Possible values are 'year', 'month', 'day', 'hour' crawlingDomMaxCheck = The maxmimum number of pages that are fetched and indexed from a single domain can be limited with this option. If combined with the 'Auto-Dom-Filter' the limit is applied to all the domains within the given depth. Domains outside the given depth are then sorted-out anyway. crawlingDomMaxPages = Integer values are allowed. If this value is given, then the maximum number of pages per domain is restricted to this given number. crawlingQ = value is either 'on' or 'off'; default is 'off'. A questionmark is usually a hint for a dynamic page. URLs pointing to dynamic content should usually not be crawled. However, there are sometimes web pages with static content that is accessed with URLs containing question marks. directDocByURL = value is either 'on' or 'off'; default is 'off'. If this is 'on', then all documents which are linked from loaded pages are indexed with their URL only and without any metadata. This applies also if these documents are outside of the crawl depth or do not match with regular expressions. storeHTCache = value is either 'on' or 'off'; default is 'off'. If 'on', all downloaded content is stored in a built-in cache, the HTCache. cachePolicy = The caching policy states when to read from the HTCache during crawling: no cache: never use the cache, all content from fresh internet source; if fresh: use the cache if the cache exists and is fresh using the proxy-fresh rules; if exist: use the cache if the cache exist. Do no check freshness. Othervise use online source; cache only: never go online, use all content from cache. If no cache exist, treat content as unavailable indexText = value is either 'on' or 'off'; default is 'off'. If this is 'on' then the fulltext is indexed. If 'off', only the metadata of the pages or media according to 'indexMedia' is recorded and indexed. indexMedia = value is either 'on' or 'off'; default is 'off'. If this is 'on' then the metadta (if available) of media content. If 'off', only the metadata of the pages or fulltext according to 'indexText' is recorded and indexed. crawlOrder = value is either 'on' or 'off'; default is 'off'. If 'on' the crawler will contact other peers and use them as remote indexers for your crawl. If crwling results are needed locally, this switch should be set to false. Only senior and principal peers can initiate or receive remote crawls. A YaCyNews message will be created to inform all peers about a global crawl, so they can omit starting a crawl with the same start point. intention = This is a text message that is posted along with the crawlOrder option to inform other YaCy peers about a distributed crawl. If 'crawlOrder' is 'off', this value is not used. xsstopw = This can be useful to circumvent that extremely common words are added to the database, i.e. \"the\", \"he\", \"she\", \"it\"... To exclude all words given in the file yacy.stopwords from indexing, this hast to be set true. collection = The name of a collection or a comma-separated list of collections. This collections can be used to separate search results into different subsets which is used with the GSA search interface using the 'site' parameter in the search request. agentName = The name of a user agent that is used for crawling. Possible values are: 'YaCy Internet (cautious)', 'YaCy Intranet (greedy)', 'Googlebot', 'Random Browser' or 'Custom Agent'. If 'Custom Agent' is used, then the user agent credentials can be set in yacy.conf with the attributes 'crawler.userAgent.name' (i.e. 'yacybot'), 'crawler.userAgent.string' (i.e. 'yacybot ($$SYSTEM$$) http://yacy.net/bot.html'), 'crawler.userAgent.minimumdelta' (i.e. '500') and 'crawler.userAgent.clienttimeout' (i.e. '10000'). The 'crawler.userAgent.name' is the token which is selected in robots.txt files for robots rules. To use these values, there must not appear the word 'yacy' anywhere in these attributes (even if they are given here in the example)","title":"YaCy Crawler API"},{"location":"operation/headless/","text":"Headless - YaCy on a Remote Server YaCy is (also) a server software which can be installed on a root server. YaCy can be remotely administrated on such a server. Either use the Debian version or the tarball release: Extract the tarball and start YaCy If you don't use the Debian version you can install YaCy anywhere, even without a root account: sudo apt-get install wget openjdk-7-jre-headless wget http://yacy.net/release/yacy_v1.80_20140916_9000.tar.gz tar xfz yacy_v1.80_20140916_9000.tar.gz cd yacy ./startYACY.sh Set a YaCy administration password The main difference during installation phase is the setting of the YACy 'admin' account: while all reqeuests coming from localhost are automatically authorized, this does not help to administrate YaCy if it is installed on a remote server because all requests to the YaCy web pages are remote. YaCy allows requests from remote within the first ten minutes and then sets a random password automatically. If this happens before you set your own administration password, you must reset the password. Set the Password before YaCy sets a Random Password Open /ConfigAccounts_p.html on your remote YaCy (i.e. if the IP of your server is 1.2.3.4, then open http://1.2.3.4:8090/ConfigAccounts_p.html), switch on \"Access only with qualified account\" and set a password in the field \"New Peer Password\". Right after you click on \"Define Administrator\" you will be requested to log in; use your new password. You can also change the password here again if you remember the old password. Re-Set the Administration Password if You Lost the Old Password This can only be done by someone who can log into the server account which is running YaCy. The following can only be done while YaCy is running! Log in into your server and then do: # cd to the YaCy home directory cd bin ./passwd.sh <mynewpassword> The account name will be 'admin', your new password is . If this does not work, try the 'Configure YaCy' option from the next section. Configure YaCy using the Linux Command Shell There are a few attributes that you can configure using the command shell, including the password. The following can only be done when YaCy is NOT running: # cd to the YaCy home directory ./reconfigureYACY.sh If this does not work for you to reset the password, try the next section. Remove the Password from yacy.conf The password is stored in hashed form in the file DATA/SETTINGS/yacy.conf The following can only be done when YaCy is NOT running: open DATA/SETTINGS/yacy.conf and do the following changes: remove property serverAccountBase64MD5 remove property adminAccount set property serverAccount to : , i.e. serverAccount=admin:mysecretpassword When YaCy is started the next time, the serverAccount is encrypted and removed to prevent that the property is there in clear text form.","title":"Headless - YaCy on a Remote Server"},{"location":"operation/headless/#headless-yacy-on-a-remote-server","text":"YaCy is (also) a server software which can be installed on a root server. YaCy can be remotely administrated on such a server. Either use the Debian version or the tarball release:","title":"Headless - YaCy on a Remote Server"},{"location":"operation/headless/#extract-the-tarball-and-start-yacy","text":"If you don't use the Debian version you can install YaCy anywhere, even without a root account: sudo apt-get install wget openjdk-7-jre-headless wget http://yacy.net/release/yacy_v1.80_20140916_9000.tar.gz tar xfz yacy_v1.80_20140916_9000.tar.gz cd yacy ./startYACY.sh","title":"Extract the tarball and start YaCy"},{"location":"operation/headless/#set-a-yacy-administration-password","text":"The main difference during installation phase is the setting of the YACy 'admin' account: while all reqeuests coming from localhost are automatically authorized, this does not help to administrate YaCy if it is installed on a remote server because all requests to the YaCy web pages are remote. YaCy allows requests from remote within the first ten minutes and then sets a random password automatically. If this happens before you set your own administration password, you must reset the password.","title":"Set a YaCy administration password"},{"location":"operation/headless/#set-the-password-before-yacy-sets-a-random-password","text":"Open /ConfigAccounts_p.html on your remote YaCy (i.e. if the IP of your server is 1.2.3.4, then open http://1.2.3.4:8090/ConfigAccounts_p.html), switch on \"Access only with qualified account\" and set a password in the field \"New Peer Password\". Right after you click on \"Define Administrator\" you will be requested to log in; use your new password. You can also change the password here again if you remember the old password.","title":"Set the Password before YaCy sets a Random Password"},{"location":"operation/headless/#re-set-the-administration-password-if-you-lost-the-old-password","text":"This can only be done by someone who can log into the server account which is running YaCy. The following can only be done while YaCy is running! Log in into your server and then do: # cd to the YaCy home directory cd bin ./passwd.sh <mynewpassword> The account name will be 'admin', your new password is . If this does not work, try the 'Configure YaCy' option from the next section.","title":"Re-Set the Administration Password if You Lost the Old Password"},{"location":"operation/headless/#configure-yacy-using-the-linux-command-shell","text":"There are a few attributes that you can configure using the command shell, including the password. The following can only be done when YaCy is NOT running: # cd to the YaCy home directory ./reconfigureYACY.sh If this does not work for you to reset the password, try the next section.","title":"Configure YaCy using the Linux Command Shell"},{"location":"operation/headless/#remove-the-password-from-yacyconf","text":"The password is stored in hashed form in the file DATA/SETTINGS/yacy.conf The following can only be done when YaCy is NOT running: open DATA/SETTINGS/yacy.conf and do the following changes: remove property serverAccountBase64MD5 remove property adminAccount set property serverAccount to : , i.e. serverAccount=admin:mysecretpassword When YaCy is started the next time, the serverAccount is encrypted and removed to prevent that the property is there in clear text form.","title":"Remove the Password from yacy.conf"},{"location":"operation/shrink/","text":"Shrink Debian by removing all graphical features to turn it into a headless server For YaCy, no online graphical interface is needed. Because YaCy starts automatically with a headless-option, only a headless java (i.e. package openjdk-7-jre-headless) is required and we can safely remove all graphics drivers from debian. Delete Packages for Graphics As root, simply do apt-get remove --purge libx11-6 followed by a clean-up apt-get autoremove --purge sudo apt-get clean This will remove many packages, but to get debian really clean, remove unused packages as well. This should include all non-headless applications. Some can be discovered automatically, see next section. Orphaned Package Removal Because some unused packages do not necessarily have dependencies to removed packages, we need a tool to discover such orphaned packages: apt-get install deborphan deborphan deborphan --guess-all You can easily remove all packages which deborphan reports with apt-get remove --purge `deborphan` apt-get remove --purge `deborphan --guess-all` apt-get autoremove --purge Repeat this until no more packages are deleted, then do a apt-get clean Remove Large Packages This goes beyond removal of non-headless software but might be useful to further shrink debian to get away large and unused software. You can easily discover large packages with dpkg-query -W --showformat='${Installed-Size} ${Package} [depends: ${Depends}]\\n' | sort -n which lists each package with occupied kilobytes, the package name and the dependencies for that package. A good starting point is the removal of large packages with many dependencies because chances are high that then again orphaned packages appear which can be removed with the 'Orphaned Package Removal' from last section. Remove Software with Large Runtime Some software on your server may consume a lot of CPU time. If some of these processes are of no use for you, you can delete them. To discover such processes, run top -bn1 | grep root | sort -k11 Whenever you removed more packages, run the 'Orphaned Package Removal' process again. Remove Old Kernel Modules Kernel modules take up a large disk space and can easily be removed. To ensure that you are doing the right thing, first check which kernel you are running with uname -r Then you can look for the modules which are stored with ls /lib/modules Finally remove the modules, which are not identical to the one as discovered with uname -r using i.e. apt-get remove --purge 3.5.0-17-generic and so on for all old kernel. This will free about 100MB of disk space for each kernel. Search for installed packages Whenever you want to remove a package, you must know the name of that package. The package names of all installed packages can be listed with dpkg --get-selections This can be useful i.e. if you want to see if you have two versions of the same tool, i.e. dpkg --get-selections *jre* may show you that you have installed jre-6 and jre-7, so you can remove one of them. You can also remove all linux source headers files, search for headers which will free a large amount of space. Prepare for tiny VirtualBox ova Appliance Dumps To further shrink the debian installation (i.e. to prepare an appliance dump in VirtualBox), it makes sense to remove more never-used applications and data. Reduce Data http://wiki.debian.org/ReduceDebian has some hints to reduce debian, this is what removes most of the remaining data: rm -Rf /usr/share/locale rm -Rf /usr/share/doc rm -Rf /usr/share/man As a very last step you can also delete the package lists rm -r /var/lib/apt/lists .. which will free about 100MB of disk space. The lists directory is recreated the next time you run apt-get update. Wipe Out Empty Space When you export a virtual machine to a ova Appliance dump, the virtual disk is compressed. To enhance this process, it is necessary to zero all empty disk space. The easiest way to do this is: dd if=/dev/zero of=/emptyspace rm -rf /emptyspace Shrink Virtual Disk Image VirtualBox has some command-line tools, this is what you would like to do to reduce the disk size after you wiped out the empty space: VBoxManage modifyhd --compact my_disk_image.vdi In case that you created the virtual machine from a ova dump, you probably don't have a vdi file but a vmdk file. This cannot be shrinked with VBoxManage, but you can create a vdi file out from the vmdk which can be shrinked then: VBoxManage clonehd --format vdi my_disk_image.vmdk my_disk_image.vdi When doing this, you must assign the new disk to the virtual machine in the VirtualBox GUI. However, a shrinked vmdk file is mostly smaller than a shrinked vdi file, therefore it does not make sense to apply this on an already shrinked appliance.","title":"Shrink Debian by removing all graphical features to turn it into a headless server"},{"location":"operation/shrink/#shrink-debian-by-removing-all-graphical-features-to-turn-it-into-a-headless-server","text":"For YaCy, no online graphical interface is needed. Because YaCy starts automatically with a headless-option, only a headless java (i.e. package openjdk-7-jre-headless) is required and we can safely remove all graphics drivers from debian.","title":"Shrink Debian by removing all graphical features to turn it into a headless server"},{"location":"operation/shrink/#delete-packages-for-graphics","text":"As root, simply do apt-get remove --purge libx11-6 followed by a clean-up apt-get autoremove --purge sudo apt-get clean This will remove many packages, but to get debian really clean, remove unused packages as well. This should include all non-headless applications. Some can be discovered automatically, see next section.","title":"Delete Packages for Graphics"},{"location":"operation/shrink/#orphaned-package-removal","text":"Because some unused packages do not necessarily have dependencies to removed packages, we need a tool to discover such orphaned packages: apt-get install deborphan deborphan deborphan --guess-all You can easily remove all packages which deborphan reports with apt-get remove --purge `deborphan` apt-get remove --purge `deborphan --guess-all` apt-get autoremove --purge Repeat this until no more packages are deleted, then do a apt-get clean","title":"Orphaned Package Removal"},{"location":"operation/shrink/#remove-large-packages","text":"This goes beyond removal of non-headless software but might be useful to further shrink debian to get away large and unused software. You can easily discover large packages with dpkg-query -W --showformat='${Installed-Size} ${Package} [depends: ${Depends}]\\n' | sort -n which lists each package with occupied kilobytes, the package name and the dependencies for that package. A good starting point is the removal of large packages with many dependencies because chances are high that then again orphaned packages appear which can be removed with the 'Orphaned Package Removal' from last section.","title":"Remove Large Packages"},{"location":"operation/shrink/#remove-software-with-large-runtime","text":"Some software on your server may consume a lot of CPU time. If some of these processes are of no use for you, you can delete them. To discover such processes, run top -bn1 | grep root | sort -k11 Whenever you removed more packages, run the 'Orphaned Package Removal' process again.","title":"Remove Software with Large Runtime"},{"location":"operation/shrink/#remove-old-kernel-modules","text":"Kernel modules take up a large disk space and can easily be removed. To ensure that you are doing the right thing, first check which kernel you are running with uname -r Then you can look for the modules which are stored with ls /lib/modules Finally remove the modules, which are not identical to the one as discovered with uname -r using i.e. apt-get remove --purge 3.5.0-17-generic and so on for all old kernel. This will free about 100MB of disk space for each kernel.","title":"Remove Old Kernel Modules"},{"location":"operation/shrink/#search-for-installed-packages","text":"Whenever you want to remove a package, you must know the name of that package. The package names of all installed packages can be listed with dpkg --get-selections This can be useful i.e. if you want to see if you have two versions of the same tool, i.e. dpkg --get-selections *jre* may show you that you have installed jre-6 and jre-7, so you can remove one of them. You can also remove all linux source headers files, search for headers which will free a large amount of space.","title":"Search for installed packages"},{"location":"operation/shrink/#prepare-for-tiny-virtualbox-ova-appliance-dumps","text":"To further shrink the debian installation (i.e. to prepare an appliance dump in VirtualBox), it makes sense to remove more never-used applications and data.","title":"Prepare for tiny VirtualBox ova Appliance Dumps"},{"location":"operation/shrink/#reduce-data","text":"http://wiki.debian.org/ReduceDebian has some hints to reduce debian, this is what removes most of the remaining data: rm -Rf /usr/share/locale rm -Rf /usr/share/doc rm -Rf /usr/share/man As a very last step you can also delete the package lists rm -r /var/lib/apt/lists .. which will free about 100MB of disk space. The lists directory is recreated the next time you run apt-get update.","title":"Reduce Data"},{"location":"operation/shrink/#wipe-out-empty-space","text":"When you export a virtual machine to a ova Appliance dump, the virtual disk is compressed. To enhance this process, it is necessary to zero all empty disk space. The easiest way to do this is: dd if=/dev/zero of=/emptyspace rm -rf /emptyspace","title":"Wipe Out Empty Space"},{"location":"operation/shrink/#shrink-virtual-disk-image","text":"VirtualBox has some command-line tools, this is what you would like to do to reduce the disk size after you wiped out the empty space: VBoxManage modifyhd --compact my_disk_image.vdi In case that you created the virtual machine from a ova dump, you probably don't have a vdi file but a vmdk file. This cannot be shrinked with VBoxManage, but you can create a vdi file out from the vmdk which can be shrinked then: VBoxManage clonehd --format vdi my_disk_image.vmdk my_disk_image.vdi When doing this, you must assign the new disk to the virtual machine in the VirtualBox GUI. However, a shrinked vmdk file is mostly smaller than a shrinked vdi file, therefore it does not make sense to apply this on an already shrinked appliance.","title":"Shrink Virtual Disk Image"},{"location":"operation/staticip/","text":"Set a static IP to a debian server This is useful after or before you installed YaCy on a debian server to get a unique handle to the YaCy peer in your network. We recommend to use the default address 192.168.1.70 to standardize the access to YaCy 'Appliance' Servers. To define a static IP in your debian installation, do edit /etc/network/interfaces replace the line iface eth0 inet dhcp with iface eth0 inet static address 192.168.1.70 netmask 255.255.255.0 gateway 192.168.1.1 If your router is at address 192.168.1.1 - otherwise replace 192.168.1.1 with the address of your router. If you want to use a different address than 192.168.1.70 for your YaCy peer, then replace 192.168.1.70 with your wanted address. If you installed YaCy with the default port 9080, then your YaCy peer will be available at http://192.168.1.70:8090 Special Setting on Ubuntu If you are running Ubuntu, then the NetworkManager will ignore these settings until you replace the line \"managed=false\" with \"managed=true\" in /etc/NetworkManager/NetworkManager.conf Set DHCP and a static IP when running debian in VirtualBox If you are running debian on a VirtualBox, then you can easily have two network interfaces. One of the interfaces can be configured as dhcp connection (over a virtual NAT) to ensure that you will always have a internet connection within your virtual machine while you can also have a second connection which makes it possible to reach your VM with an IP within your intranet (or from the console where you are running the VM). in the VirtualBox manager, open the network configuration and set Adapter 1 to NAT set Adapter 2 to Bridge with name: en1 in /etc/network/interfaces, set the configuration to: auto lo iface lo inet loopback allow-hotplug eth0 iface eth0 inet dhcp allow-hotplug eth1 iface eth1 inet static address 192.168.1.70 netmask 255.255.255.0 gateway 192.168.1.1 This will enable access to the VM at port 192.168.1.70. Please replace this IP to the value you want.","title":"Set a static IP to a debian server"},{"location":"operation/staticip/#set-a-static-ip-to-a-debian-server","text":"This is useful after or before you installed YaCy on a debian server to get a unique handle to the YaCy peer in your network. We recommend to use the default address 192.168.1.70 to standardize the access to YaCy 'Appliance' Servers. To define a static IP in your debian installation, do edit /etc/network/interfaces replace the line iface eth0 inet dhcp with iface eth0 inet static address 192.168.1.70 netmask 255.255.255.0 gateway 192.168.1.1 If your router is at address 192.168.1.1 - otherwise replace 192.168.1.1 with the address of your router. If you want to use a different address than 192.168.1.70 for your YaCy peer, then replace 192.168.1.70 with your wanted address. If you installed YaCy with the default port 9080, then your YaCy peer will be available at http://192.168.1.70:8090","title":"Set a static IP to a debian server"},{"location":"operation/staticip/#special-setting-on-ubuntu","text":"If you are running Ubuntu, then the NetworkManager will ignore these settings until you replace the line \"managed=false\" with \"managed=true\" in /etc/NetworkManager/NetworkManager.conf","title":"Special Setting on Ubuntu"},{"location":"operation/staticip/#set-dhcp-and-a-static-ip-when-running-debian-in-virtualbox","text":"If you are running debian on a VirtualBox, then you can easily have two network interfaces. One of the interfaces can be configured as dhcp connection (over a virtual NAT) to ensure that you will always have a internet connection within your virtual machine while you can also have a second connection which makes it possible to reach your VM with an IP within your intranet (or from the console where you are running the VM). in the VirtualBox manager, open the network configuration and set Adapter 1 to NAT set Adapter 2 to Bridge with name: en1 in /etc/network/interfaces, set the configuration to: auto lo iface lo inet loopback allow-hotplug eth0 iface eth0 inet dhcp allow-hotplug eth1 iface eth1 inet static address 192.168.1.70 netmask 255.255.255.0 gateway 192.168.1.1 This will enable access to the VM at port 192.168.1.70. Please replace this IP to the value you want.","title":"Set DHCP and a static IP when running debian in VirtualBox"}]}